# COL764 – Assignment 2

This repository implements boolean phrase search, VSM, and BM25 over the provided CORD-19 subset. Tokenization uses spaCy’s default English tokenizer only. Stemming and lemmatization are **not** used. A stopwords file path is accepted by the scripts but is **ignored** in code.

## Environment

- Python 3.12
- Dependencies: Python standard library + `spacy` (tokenizer only)
- No internet is required at build time.

## Project Layout

- `tokenize_corpus.py` – Task 0 (spaCy tokenizer → `vocab.txt`)
- `build_index.py` – Task 1 (positional inverted index → `index.json`; sidecars: `vsm.json`, `bm25.json`)
- `phrase_search.py` – Task 2 (boolean phrase search → `phrase_search_docids.txt`)
- `vsm.py` – Task 3 (tf–idf + cosine → `vsm_docids.txt`)
- `bm25_retrieval.py` – Task 4 (Okapi BM25 → `bm25_docids.txt`)
- Shell scripts:
  - `build.sh`
  - `tokenize_corpus.sh`
  - `build_index.sh`
  - `phrase_search.sh`
  - `vsm.sh`
  - `bm25_retrieval.sh`
  - `feedback.sh` (**placeholder**, not yet implemented)
- Figures (to be added later): `heatmap_k20.png`, `heatmap_k200.png`

## Run Steps

Mirror the assignment order. Replace placeholders with your paths.

```bash
bash build.sh
bash tokenize_corpus.sh <CORPUS_DIR> <PATH_OF_STOPWORD_FILE> <VOCAB_DIR>
bash build_index.sh <CORPUS_DIR> <VOCAB_PATH> <INDEX_DIR>
bash phrase_search.sh <INDEX_DIR> <QUERY_FILE_PATH> <OUTPUT_DIR> <PATH_OF_STOPWORDS_FILE>
bash vsm.sh <INDEX_DIR> <QUERY_FILE_PATH> <OUTPUT_DIR> <PATH_OF_STOPWORDS_FILE> 20
bash vsm.sh <INDEX_DIR> <QUERY_FILE_PATH> <OUTPUT_DIR> <PATH_OF_STOPWORDS_FILE> 200
bash bm25_retrieval.sh <INDEX_DIR> <QUERY_FILE_PATH> <OUTPUT_DIR> <PATH_OF_STOPWORDS_FILE> 20
bash bm25_retrieval.sh <INDEX_DIR> <QUERY_FILE_PATH> <OUTPUT_DIR> <PATH_OF_STOPWORDS_FILE> 200
# (later) bash feedback.sh <...> 20
# (later) bash feedback.sh <...> 200
```

Notes:
- Each script also accepts an optional `--time` flag at the end to print timing (see provided examples/comments).
- The stopwords argument is accepted but **ignored** by all programs (tokenizer-only pipeline).

## Outputs

Each retrieval program writes a TREC run file (one line per result):

```
qid docid rank score
```

Expected files:
- `phrase_search_docids.txt`
- `vsm_docids.txt`
- `bm25_docids.txt`
- (later) `feedback_docids.txt`

## Evaluation

Use your evaluator to compute Precision, Recall, and F1 against the provided qrels. Example:

```bash
python3 Scoring/evaluate_ir.py --qrels <qrels.json> --run <run_file>
```

Fill the resulting P/R/F1 numbers into the tables in `report.tex` (both @20 and @200 for VSM and BM25, and overall for phrase search).

## Implementation Notes

- Tokenizer: `spacy.blank("en")` only; no stemming/lemmatization. Stopwords path is ignored.
- Index: terms and docIDs serialized lexicographically; postings store `{tf, pos[]}` with 0-based positions.
- Sidecar stats for ranked retrieval are generated by `build_index.py` into `vsm.json` and `bm25.json`.
- PRF (`feedback.sh`) is planned as one-round pseudo-relevance feedback over VSM and will be added later.
```